{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT IDS Model Training and Saving\n",
    "This notebook trains the best LightGBM model and saves it as a pickle file for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install lightgbm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset - UPDATE THIS PATH\n",
    "# For Kaggle: /kaggle/input/your-dataset-name/filename.csv\n",
    "data_path = \"/kaggle/input/iotcic-dataset/iot_dataset.csv\"  # Update this!\n",
    "\n",
    "print(\"üìä Loading dataset...\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create taxonomy mapping\n",
    "taxonomy_map = {\n",
    "    # Flood Attacks\n",
    "    'DoS-TCP_Flood': 'Flood Attacks',\n",
    "    'DoS-UDP_Flood': 'Flood Attacks', \n",
    "    'DoS-SYN_Flood': 'Flood Attacks',\n",
    "    'DoS-HTTP_Flood': 'Flood Attacks',\n",
    "    \n",
    "    # Botnet/Mirai Attacks\n",
    "    'Mirai-greeth_flood': 'Botnet/Mirai Attacks',\n",
    "    'Mirai-greip_flood': 'Botnet/Mirai Attacks',\n",
    "    'Mirai-udpplain': 'Botnet/Mirai Attacks',\n",
    "    \n",
    "    # Spoofing/MITM\n",
    "    'MITM-ArpSpoofing': 'Spoofing / MITM',\n",
    "    'DNS_Spoofing': 'Spoofing / MITM',\n",
    "    \n",
    "    # Reconnaissance  \n",
    "    'Recon-PingSweep': 'Reconnaissance',\n",
    "    'Recon-OSScan': 'Reconnaissance',\n",
    "    'Recon-PortScan': 'Reconnaissance',\n",
    "    'VulnerabilityScan': 'Reconnaissance',\n",
    "    \n",
    "    # Backdoors & Exploits\n",
    "    'Backdoor_Malware': 'Backdoors & Exploits',\n",
    "    'BrowserHijacking': 'Backdoors & Exploits',\n",
    "    'CommandInjection': 'Backdoors & Exploits',\n",
    "    \n",
    "    # Injection Attacks\n",
    "    'SqlInjection': 'Injection Attacks',\n",
    "    'XSS': 'Injection Attacks',\n",
    "    \n",
    "    # Benign\n",
    "    'BenignTraffic': 'Benign'\n",
    "}\n",
    "\n",
    "print(f\"Taxonomy mapping created with {len(taxonomy_map)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "print(\"üîß Preparing data...\")\n",
    "\n",
    "# Drop rows with missing labels\n",
    "df_clean = df.dropna(subset=['label']).copy()\n",
    "print(f\"Clean dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Apply taxonomy mapping\n",
    "df_clean['taxonomy_label'] = df_clean['label'].map(taxonomy_map).fillna('Unknown')\n",
    "\n",
    "# Prepare features (drop label columns)\n",
    "X = df_clean.drop(columns=['label', 'taxonomy_label'])\n",
    "y = df_clean['taxonomy_label']\n",
    "\n",
    "# Keep only numeric columns\n",
    "numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
    "X = X[numeric_columns]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels and scale features\n",
    "print(\"‚öôÔ∏è Encoding and scaling...\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Scaled features shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "print(\"üöÄ Training LightGBM model...\")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=len(le.classes_),\n",
    "    is_unbalance=True,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=10,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"üìä Evaluating model...\")\n",
    "\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model pipeline\n",
    "print(\"üíæ Saving model pipeline...\")\n",
    "\n",
    "# Create timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Model pipeline dictionary\n",
    "model_pipeline = {\n",
    "    'model': lgb_model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': le,\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'model_type': 'LightGBM',\n",
    "    'timestamp': timestamp,\n",
    "    'taxonomy_map': taxonomy_map,\n",
    "    'accuracy': accuracy,\n",
    "    'f1_score': f1\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "pickle_filename = f\"iot_ids_lightgbm_{timestamp}.pkl\"\n",
    "with open(pickle_filename, 'wb') as f:\n",
    "    pickle.dump(model_pipeline, f)\n",
    "\n",
    "# Save with joblib (alternative)\n",
    "joblib_filename = f\"iot_ids_lightgbm_{timestamp}.joblib\"\n",
    "joblib.dump(model_pipeline, joblib_filename)\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully!\")\n",
    "print(f\"Pickle file: {pickle_filename}\")\n",
    "print(f\"Joblib file: {joblib_filename}\")\n",
    "print(f\"File sizes:\")\n",
    "print(f\"  Pickle: {os.path.getsize(pickle_filename) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Joblib: {os.path.getsize(joblib_filename) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model info file\n",
    "info_filename = f\"model_info_{timestamp}.txt\"\n",
    "with open(info_filename, 'w') as f:\n",
    "    f.write(f\"IoT IDS Model Information\\n\")\n",
    "    f.write(f\"========================\\n\")\n",
    "    f.write(f\"Model Type: LightGBM Classifier\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {f1:.4f}\\n\")\n",
    "    f.write(f\"Features: {len(X.columns)}\\n\")\n",
    "    f.write(f\"Classes: {len(le.classes_)}\\n\")\n",
    "    f.write(f\"Class Names: {', '.join(le.classes_)}\\n\")\n",
    "    f.write(f\"\\nTaxonomy Mapping:\\n\")\n",
    "    for original, taxonomy in taxonomy_map.items():\n",
    "        f.write(f\"  {original} -> {taxonomy}\\n\")\n",
    "\n",
    "print(f\"üìÑ Model info saved to: {info_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "print(\"üîç Testing model loading...\")\n",
    "\n",
    "# Load the model\n",
    "with open(pickle_filename, 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "\n",
    "loaded_model = loaded_pipeline['model']\n",
    "loaded_scaler = loaded_pipeline['scaler']\n",
    "loaded_le = loaded_pipeline['label_encoder']\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_test[:1]\n",
    "test_scaled = loaded_scaler.transform(test_sample)\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "prediction_proba = loaded_model.predict_proba(test_scaled)\n",
    "\n",
    "class_name = loaded_le.inverse_transform(prediction)[0]\n",
    "confidence = max(prediction_proba[0])\n",
    "\n",
    "print(f\"‚úÖ Model loaded and tested successfully!\")\n",
    "print(f\"Test prediction: {class_name} (confidence: {confidence:.4f})\")\n",
    "print(f\"\\nüéâ Your IoT IDS model is ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
